{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "868f20a6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Predicting Hospital Readmissions Using Integrated Patient, Clinical, and Socioeconomic Data\n",
    " \n",
    "1.2.1\tðŸŽ¯ Project Objective:\n",
    "To develop a predictive model for 30-day hospital readmission risk by merging and cleaning patient demographics, clinical encounter data, and socioeconomic data. The goal is to help hospitals reduce readmissions, improve patient outcomes, and reduce costs.\n",
    "### J. Casey Brookshier\n",
    "### 7/21/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa7e4f5-d0d6-4cbc-aa31-f853a9e32fdc",
   "metadata": {},
   "source": [
    "## \"Hospital Quality Forecasting: Data-Driven Insights into Readmission Penalties\"\n",
    "Recommended Workflow: Clean First, Then Integrate\n",
    "## In short: Clean â†’ Standardize â†’ Aggregate â†’ Integrate â†’ Analyze\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effd1e07-b651-4f91-80ba-5baa60cdbed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hospital Readmission Risk Forecasting\n",
    "\n",
    "## Objective\n",
    "Predict hospital-level 30-day readmission risk using publicly available\n",
    "CMS readmission metrics, healthcare-associated infection indicators,\n",
    "and socioeconomic deprivation (ADI).\n",
    "\n",
    "## Business Value\n",
    "â€¢ Identify facilities at risk of CMS readmission penalties  \n",
    "â€¢ Support targeted quality improvement initiatives  \n",
    "â€¢ Enable data-informed policy and administrative decisions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b948b009-0205-405b-9a43-514b720a8b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_readmission_forecasting/\n",
    "â”‚\n",
    "â”œâ”€â”€ data/\n",
    "â”‚   â”œâ”€â”€ FY_2025_Hospital_Readmissions_Reduction_Program_Hospital.csv\n",
    "â”‚   â”œâ”€â”€ Healthcare_Associated_Infections-Hospital.csv\n",
    "â”‚   â”œâ”€â”€ CO_2023_ADI_9 Digit Zip Code_v4_0_1.csv\n",
    "â”‚   â””â”€â”€ hospital_readmissions_analytic_table.csv  \n",
    "â”‚\n",
    "â”œâ”€â”€ artifacts/\n",
    "â”‚   â”œâ”€â”€ random_forest_model.pkl\n",
    "â”‚   â”œâ”€â”€ feature_names.pkl\n",
    "â”‚   â””â”€â”€ imputer.pkl\n",
    "â”‚\n",
    "â”œâ”€â”€ src/\n",
    "â”‚   â”œâ”€â”€ prepare_data.py        \n",
    "â”‚   â””â”€â”€ train_readmissions_model.py\n",
    "â”‚\n",
    "â”œâ”€â”€ requirements.txt\n",
    "â””â”€â”€ README.md\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e095511a-8d14-4e7d-95d9-467ebc7dbd81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d65b414-aace-414f-8900-5486e5598e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41168b76-4032-4e89-83d5-328df2dd7cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b6e81d-84b0-4e98-8205-cdd84bc8bd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Hospital Readmission Forecasting â€“ Data Preparation Script\n",
    "# ============================================================\n",
    "# Author: J. Casey Brookshier\n",
    "# Purpose: Build analytic dataset for readmission modeling\n",
    "# Inputs: Raw CMS Readmissions, Infections, ADI files\n",
    "# Output: hospital_readmissions_analytic_table.csv\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG (RELATIVE PATHS)\n",
    "# ============================================================\n",
    "# find project root dynamically (relative paths for location independent functionality)\n",
    "PROJECT_ROOT = Path(__file__).resolve().parents[1]\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "# define where merged dataset to be written\n",
    "OUTPUT_FILE = DATA_DIR / \"hospital_readmissions_analytic_table.csv\"\n",
    "\n",
    "# define filepaths for raw data sources\n",
    "READMISSIONS_FILE = DATA_DIR / \"FY_2025_Hospital_Readmissions_Reduction_Program_Hospital.csv\"\n",
    "INFECTIONS_FILE   = DATA_DIR / \"Healthcare_Associated_Infections-Hospital.csv\"\n",
    "ADI_FILE          = DATA_DIR / \"CO_2023_ADI_9 Digit Zip Code_v4_0_1.csv\"\n",
    "\n",
    "# Define readmission columns of interest, centralize metric control\n",
    "READMISSION_METRICS = [\n",
    "    \"Excess Readmission Ratio\",\n",
    "    \"Predicted Readmission Rate\",\n",
    "    \"Expected Readmission Rate\",\n",
    "]\n",
    "\n",
    "# log confirmation of start of pipeline execution\n",
    "print(\"ðŸ”§ Rebuilding analytic dataset from raw sources\")\n",
    "\n",
    "# ============================================================\n",
    "# LOAD DATA -> read CSV's into pandas DF\n",
    "# ============================================================\n",
    "\n",
    "readm_df = pd.read_csv(READMISSIONS_FILE)\n",
    "infect_df = pd.read_csv(INFECTIONS_FILE)\n",
    "adi_df    = pd.read_csv(ADI_FILE)\n",
    "\n",
    "# ============================================================\n",
    "# CANONICAL KEY NORMALIZATION -> force Facility ID into string\n",
    "# ============================================================\n",
    "\n",
    "readm_df[\"Facility ID\"] = readm_df[\"Facility ID\"].astype(str)\n",
    "infect_df[\"Facility ID\"] = infect_df[\"Facility ID\"].astype(str)\n",
    "# normalize ZIP into 5 digit strings for reliable ZIP/ADI join\n",
    "infect_df[\"ZIP Code\"] = infect_df[\"ZIP Code\"].astype(str).str.zfill(5)\n",
    "\n",
    "# ============================================================\n",
    "# CLEAN READMISSIONS DATA\n",
    "# ============================================================\n",
    "# create working copy\n",
    "readm = readm_df.copy()\n",
    "\n",
    "# convert verbose CMS names into clean feature identifiers\n",
    "readm[\"measure_code\"] = (\n",
    "    readm[\"Measure Name\"]\n",
    "    .str.replace(\"READM-30-\", \"\", regex=False)\n",
    "    .str.replace(\"-HRRP\", \"\", regex=False)\n",
    "    .str.lower()\n",
    ")\n",
    "# convert readmission metric to numeric, invalid values to NaN\n",
    "for col in READMISSION_METRICS:\n",
    "    readm[col] = pd.to_numeric(readm[col], errors=\"coerce\")\n",
    "    \n",
    "# remove hospital w/o valid readmission signal\n",
    "readm = readm.dropna(subset=[\"Excess Readmission Ratio\"])\n",
    "\n",
    "# reshape data long ->wide, one facility one row, one condition one column\n",
    "readm_pivot = readm.pivot_table(\n",
    "    index=[\"Facility ID\", \"Facility Name\", \"State\"],\n",
    "    columns=\"measure_code\",\n",
    "    values=READMISSION_METRICS\n",
    ")\n",
    "\n",
    "# flatten pandas' multi-index columns, create ML-ready feature names\n",
    "readm_pivot.columns = [\n",
    "    f\"{metric.replace(' ', '_').lower()}__{measure}\"\n",
    "    for metric, measure in readm_pivot.columns\n",
    "]\n",
    "# change index fields back into columns\n",
    "readm_pivot = readm_pivot.reset_index()\n",
    "\n",
    "# check for facility ID pipeline corruption\n",
    "assert \"Facility ID\" in readm_pivot.columns, \"Facility ID missing after pivot\"\n",
    "\n",
    "# ============================================================\n",
    "# CLEAN INFECTIONS DATA\n",
    "# ============================================================\n",
    "# working copy\n",
    "infect = infect_df.copy()\n",
    "\n",
    "# infection scores to numeric, remove invalid rows\n",
    "infect[\"Score\"] = pd.to_numeric(infect[\"Score\"], errors=\"coerce\")\n",
    "infect = infect.dropna(subset=[\"Score\"])\n",
    "\n",
    "# standardize measure names into clean column identifiers\n",
    "infect[\"measure_clean\"] = (\n",
    "    infect[\"Measure Name\"]\n",
    "    .str.replace(\"[^A-Za-z0-9]+\", \"_\", regex=True)\n",
    "    .str.lower()\n",
    ")\n",
    "\n",
    "# aggregate infection metrics per hospital\n",
    "infect_pivot = infect.pivot_table(\n",
    "    index=\"Facility ID\",\n",
    "    columns=\"measure_clean\",\n",
    "    values=\"Score\",\n",
    "    aggfunc=\"mean\"\n",
    ").add_prefix(\"infection__\")\n",
    "\n",
    "# prep for merge\n",
    "infect_pivot = infect_pivot.reset_index()\n",
    "\n",
    "# ============================================================\n",
    "# CLEAN ADI DATA \n",
    "# ============================================================\n",
    "# improve column readability\n",
    "adi = adi_df.rename(columns={\n",
    "    \"ZIP_4\": \"zip\",\n",
    "    \"ADI_NATRANK\": \"adi_national\",\n",
    "    \"ADI_STATERNK\": \"adi_state\",\n",
    "})\n",
    "\n",
    "# Keep only required columns to avoid dtype pollution\n",
    "adi = adi[[\"zip\", \"adi_national\", \"adi_state\"]]\n",
    "\n",
    "# norm ZIP to 5 value string\n",
    "adi[\"zip\"] = adi[\"zip\"].astype(str).str.zfill(5)\n",
    "\n",
    "# adi values numeric/valid\n",
    "adi[\"adi_national\"] = pd.to_numeric(adi[\"adi_national\"], errors=\"coerce\")\n",
    "adi[\"adi_state\"] = pd.to_numeric(adi[\"adi_state\"], errors=\"coerce\")\n",
    "adi = adi.dropna(subset=[\"adi_national\", \"adi_state\"])\n",
    "adi = adi.astype({\n",
    "    \"adi_national\": \"float64\",\n",
    "    \"adi_state\": \"float64\",\n",
    "})\n",
    "# aggregate 9 digit zip to 5 digit\n",
    "adi_zip = (\n",
    "    adi.groupby(\"zip\", as_index=False)\n",
    "       .mean(numeric_only=True)\n",
    ")\n",
    "# verify aggregation produces numeric\n",
    "assert adi_zip[[\"adi_national\", \"adi_state\"]].apply(\n",
    "    lambda s: np.issubdtype(s.dtype, np.number)\n",
    ").all(), \"ADI aggregation produced non-numeric output\"\n",
    "\n",
    "# ============================================================\n",
    "# FACILITY â†’ ZIP â†’ ADI BRIDGE\n",
    "# ============================================================\n",
    "# build facility to ZIP lookup table\n",
    "facility_zip = (\n",
    "    infect_df[[\"Facility ID\", \"ZIP Code\"]]\n",
    "    .drop_duplicates()\n",
    "    .rename(columns={\"ZIP Code\": \"zip\"})\n",
    ")\n",
    "facility_zip[\"Facility ID\"] = facility_zip[\"Facility ID\"].astype(str)\n",
    "facility_zip[\"zip\"] = facility_zip[\"zip\"].astype(str).str.zfill(5)\n",
    "\n",
    "# attach SES context by facility\n",
    "facility_adi = facility_zip.merge(\n",
    "    adi_zip,\n",
    "    on=\"zip\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "assert \"Facility ID\" in facility_adi.columns, \"Facility ID missing in facility_adi\"\n",
    "\n",
    "# ============================================================\n",
    "# FINAL ANALYTIC TABLE -> readmissions + infections +SES data\n",
    "# ============================================================\n",
    "# create merged ML ready DF\n",
    "final_df = (\n",
    "    readm_pivot\n",
    "    .merge(infect_pivot, on=\"Facility ID\", how=\"left\")\n",
    "    .merge(\n",
    "        facility_adi[[\"Facility ID\", \"adi_national\", \"adi_state\"]],\n",
    "        on=\"Facility ID\",\n",
    "        how=\"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# TARGET CONSTRUCTION\n",
    "# ============================================================\n",
    "# identify excess readmission metrics\n",
    "excess_cols = [\n",
    "    c for c in final_df.columns\n",
    "    if c.startswith(\"excess_readmission_ratio__\")\n",
    "]\n",
    "# create model target, AVE excess readmission\n",
    "final_df[\"composite_readmission_score\"] = final_df[excess_cols].mean(axis=1)\n",
    "\n",
    "# ============================================================\n",
    "# FINAL VALIDATION -> raise error if key columns missing\n",
    "# ============================================================\n",
    "\n",
    "REQUIRED_COLUMNS = {\n",
    "    \"Facility ID\",\n",
    "    \"Facility Name\",\n",
    "    \"State\",\n",
    "    \"composite_readmission_score\",\n",
    "}\n",
    "missing = REQUIRED_COLUMNS - set(final_df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns in final dataset: {missing}\")\n",
    "\n",
    "# ============================================================\n",
    "# SAVE OUTPUT, write analytic dataset to disk, report size\n",
    "# ============================================================\n",
    "\n",
    "final_df.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(f\"âœ… Analytic dataset saved: {OUTPUT_FILE.resolve()}\")\n",
    "print(f\"ðŸ“¦ Final shape: {final_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68cfbda-1938-42d8-b9ab-a255d333fe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# train_readmissions_model.py\n",
    "# ============================================================\n",
    "# Purpose:\n",
    "#   Train and evaluate readmission risk models using the\n",
    "#   prepared analytic dataset.\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "# create train/test datasets, perform k-fold cross-validation\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "# create baseline linear regression model, random forest model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# model evaluation metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "# handle missing values -> replace with column means\n",
    "from sklearn.impute import SimpleImputer\n",
    "# serialize python objects to disk\n",
    "import pickle\n",
    "\n",
    "# ============================================================\n",
    "# PATHS -> # find project root dynamically \n",
    "# (relative paths for location independent functionality)\n",
    "# ============================================================\n",
    "\n",
    "PROJECT_ROOT = Path(__file__).resolve().parents[1\n",
    "# define where input data is and where trained models saved\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "ARTIFACT_DIR = PROJECT_ROOT / \"artifacts\"\n",
    "# define path to dataset created by prev script\n",
    "DATA_FILE = DATA_DIR / \"hospital_readmissions_analytic_table.csv\"\n",
    "\n",
    "# ============================================================\n",
    "# LOAD DATA -> load merged data set into df\n",
    "# ============================================================\n",
    "\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "\n",
    "# define relevant columns for effective modeling\n",
    "REQUIRED_COLUMNS = {\n",
    "    \"Facility ID\",\n",
    "    \"Facility Name\",\n",
    "    \"State\",\n",
    "    \"composite_readmission_score\",\n",
    "}\n",
    "\n",
    "missing = REQUIRED_COLUMNS - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Required column(s) missing: {missing}\")\n",
    "\n",
    "print(f\"âœ… Loaded dataset: {df.shape}\")\n",
    "\n",
    "# ============================================================\n",
    "# MODEL PREP -> define prediction target (dependent variable)\n",
    "# ============================================================\n",
    "\n",
    "TARGET = \"composite_readmission_score\"\n",
    "\n",
    "# create feature matrix x, minus non-pred identifiers, target\n",
    "X = df.drop(columns=[\n",
    "    \"Facility ID\",\n",
    "    \"Facility Name\",\n",
    "    \"State\",\n",
    "    TARGET,\n",
    "])\n",
    "\n",
    "# extract target vector\n",
    "y = df[TARGET]\n",
    "\n",
    "# remove missing columns\n",
    "X = X.dropna(axis=1, how=\"all\")\n",
    "\n",
    "# implement mean value imputer for missing values\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "\n",
    "# fit imputer on full dataset, wrap output into DF w/ proper labels\n",
    "X = pd.DataFrame(\n",
    "    imputer.fit_transform(X),\n",
    "    columns=X.columns,\n",
    "    index=X.index,\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# TRAIN / TEST -> 80/20 train/test split\n",
    "# ============================================================\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# instantiate LR model, fit to train data\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# 100 trees, reproducible randomness, use all CPU cores\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "rf.fit(X_train, y_train) #train random forest\n",
    "\n",
    "# ============================================================\n",
    "# EVALUATION -> function to predict on test data\n",
    "# compute RMSE, R^2, return metrics as dictionary\n",
    "# ============================================================\n",
    "\n",
    "def evaluate(model):\n",
    "    preds = model.predict(X_test)\n",
    "    return {\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_test, preds)),\n",
    "        \"R2\": r2_score(y_test, preds),\n",
    "    }\n",
    "\n",
    "# LR & random forest eval on test set, print results\n",
    "print(\"\\nðŸ“Š Model Performance\")\n",
    "print(\"Linear Regression:\", evaluate(lr))\n",
    "print(\"Random Forest:\", evaluate(rf))\n",
    "\n",
    "# perform 5-fold cross validation on entire dataset\n",
    "cv_rmse = np.sqrt(\n",
    "    -cross_val_score(\n",
    "        rf,\n",
    "        X,\n",
    "        y,\n",
    "        cv=5,\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# print ave RMSE, variability of model performance\n",
    "print(\"\\nðŸ“ˆ Random Forest CV RMSE\")\n",
    "print(\"Mean:\", round(cv_rmse.mean(), 4), \"Std:\", round(cv_rmse.std(), 4))\n",
    "\n",
    "# ============================================================\n",
    "# SAVE ARTIFACTS -> create artifacts/dir if it doesn't exist\n",
    "# save trained RF mod, feature names, fitted imputer for \n",
    "# consistent missing values\n",
    "# ============================================================\n",
    "\n",
    "ARTIFACT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "with open(ARTIFACT_DIR / \"random_forest_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(rf, f)\n",
    "\n",
    "with open(ARTIFACT_DIR / \"feature_names.pkl\", \"wb\") as f:\n",
    "    pickle.dump(list(X.columns), f)\n",
    "\n",
    "with open(ARTIFACT_DIR / \"imputer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(imputer, f)\n",
    "\n",
    "print(\"\\nâœ… Model artifacts saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710cdd6c-ad55-4eea-9e17-bd251be4c784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ccc1b7-1743-4049-b037-c18d32a35cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ae7643-6890-4591-8234-13365c378a35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f63d1f-f7e9-4dc6-9f95-b1893cc8f2ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99ea83d-bf26-491a-92c5-49aebfff13b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
